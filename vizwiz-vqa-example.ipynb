{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Joint Embedding model on the Vizwiz VQA dataset\n\nThis code is based on this great article here:\n\nhttps://medium.com/data-science-at-microsoft/visual-question-answering-with-multimodal-transformers-d4f57950c867\n\nWith (not limited to) the following modifications:\n- Adapted to work with the Vizwiz VQA dataset\n- Several model changes, the important ones are initialization and the loss function (Focal Loss)\n- Focal loss has the option to add uniform label smoothing\n- Different way of selecting target label\n- Added script to load external data (COCO)\n- Added the VQA accuracy metric and can be computed while training\n\nBe aware that:\n- This code has a cpu bottleneck and should be faster if solved, for example, preprocessing the images in another script instead of preprocessing it on the fly.\n- For convience some parts of this code relies on global variables such as answer_space and the indices of the validation dataframe.","metadata":{}},{"cell_type":"code","source":"# Path configs\nfrom pathlib import Path\n\nANNOTATIONS_BASE_PATH = \"/kaggle/input/vizwiz/Annotations/Annotations/\"\nIMAGES_TRAIN_PATH = \"/kaggle/input/vizwiz/train/train/\"\nIMAGES_VAL_PATH = \"/kaggle/input/vizwiz/val/val/\"\nANSWER_SPACE_EMBED_PATH = \"/kaggle/input/answer-space-embed/answer_embed.pt\"\nANNOTATIONS_TRAIN_PATH = str(Path(ANNOTATIONS_BASE_PATH)/\"train.json\")\nANNOTATIONS_VAL_PATH = str(Path(ANNOTATIONS_BASE_PATH)/\"val.json\")\n\nANSWER_SPACE_SIZE = 3000","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-15T09:20:38.189428Z","iopub.execute_input":"2022-09-15T09:20:38.189867Z","iopub.status.idle":"2022-09-15T09:20:38.217489Z","shell.execute_reply.started":"2022-09-15T09:20:38.189779Z","shell.execute_reply":"2022-09-15T09:20:38.216431Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\nfrom datasets import load_dataset, set_caching_enabled\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom transformers import (\n    # Preprocessing / Common\n    AutoTokenizer, AutoFeatureExtractor,\n    # Text & Image Models (Now, image transformers like ViTModel, DeiTModel, BEiT can also be loaded using AutoModel)\n    AutoModel,            \n    # Training / Evaluation\n    TrainingArguments, Trainer,\n    # Misc\n    logging\n)\n\nfrom sklearn.metrics import accuracy_score, f1_score\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:38.219442Z","iopub.execute_input":"2022-09-15T09:20:38.220335Z","iopub.status.idle":"2022-09-15T09:20:47.518970Z","shell.execute_reply.started":"2022-09-15T09:20:38.220288Z","shell.execute_reply":"2022-09-15T09:20:47.517768Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# SET CACHE FOR HUGGINGFACE TRANSFORMERS + DATASETS\nos.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n# SET ONLY 1 GPU DEVICE\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nset_caching_enabled(True)\nlogging.set_verbosity_error()\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n#     print('Memory Usage:')\n#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n#     print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:47.520248Z","iopub.execute_input":"2022-09-15T09:20:47.523252Z","iopub.status.idle":"2022-09-15T09:20:47.629371Z","shell.execute_reply.started":"2022-09-15T09:20:47.523208Z","shell.execute_reply":"2022-09-15T09:20:47.628462Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda:0\nTesla P100-PCIE-16GB\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n  \n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport datetime\nfrom collections import Counter\n\nclass VQAData:\n    def __init__(self, annotation_fp, image_base_path):\n        # Read in json file as pd.dataframe\n        self.df = pd.read_json(annotation_fp,orient=\"records\")\n        print(\"Read in file: {}\".format(annotation_fp))\n        print(\"File has shape: {}\".format(self.df.shape))\n        \n        def most_common(lst):\n            data = Counter(lst)\n            return max(lst, key=data.get)\n        \n        # max_answer is the most common answer from the ten answers provided\n        # \n        self.df[\"max_answer\"] = self.df[\"answers\"].apply(lambda row:most_common([ans[\"answer\"] for ans in row]))\n        self.df[\"max_answer_confidence\"] = self.df[\"answers\"].apply(lambda row:most_common([ans[\"answer_confidence\"] for ans in row]))\n        self.df[\"answer_list\"] = self.df[\"answers\"].apply(lambda row:[ans[\"answer\"] for ans in row])\n        image_base_path = Path(image_base_path)\n        self.df[\"image_path\"] = self.df[\"image\"].apply(lambda row:str(image_base_path/row))\n        \n    def get_df(self,fields=[\"image_path\",\"question\",\"max_answer\"], answer_type=None):\n        if(answer_type!=None):\n            _df = self.df[self.df[\"answer_type\"]==answer_type].reset_index(drop=True)\n        else:\n            _df = self.df.copy()\n        return(_df[fields])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:47.631891Z","iopub.execute_input":"2022-09-15T09:20:47.632216Z","iopub.status.idle":"2022-09-15T09:20:47.658820Z","shell.execute_reply.started":"2022-09-15T09:20:47.632187Z","shell.execute_reply":"2022-09-15T09:20:47.657862Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"vqa_train = VQAData(ANNOTATIONS_TRAIN_PATH,IMAGES_TRAIN_PATH)\nvqa_val = VQAData(ANNOTATIONS_VAL_PATH,IMAGES_VAL_PATH)\ntrain_df = vqa_train.get_df(fields=[\"image_path\",\"question\",\"max_answer\",\"answer_list\"])\nval_df = vqa_val.get_df(fields=[\"image_path\",\"question\",\"max_answer\",\"answer_list\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:47.665318Z","iopub.execute_input":"2022-09-15T09:20:47.667657Z","iopub.status.idle":"2022-09-15T09:20:49.245767Z","shell.execute_reply.started":"2022-09-15T09:20:47.667620Z","shell.execute_reply":"2022-09-15T09:20:49.243575Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Read in file: /kaggle/input/vizwiz/Annotations/Annotations/train.json\nFile has shape: (20523, 5)\nRead in file: /kaggle/input/vizwiz/Annotations/Annotations/val.json\nFile has shape: (4319, 5)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Why cast \"unsuitable image\" -> \"unsuitable\"?\nIn the paper that presented the VizWiz dataset, it defined \n- Unsuitable Image: an image is too poor in quality to answer the question (i.e., all white, all black, or too blurry)\n- Unanswerable: the question cannot be answered from the image\n\nBut there is a high portion of answers labeled with \"unsuitable\" which doesn't lay in either category and is a significant noise to the model causing the accuracy to degrade. (Simply thought, even if the model is 100% sure that this is a unsuitable image, it still has a 50% to get it wrong because of the mixing of \"unsuitable image\" and \"unsuitable\" labels\")\n\nIn this notebook we modify both the train/validation set labels to get a clear idea of how the model is training, but didn't modify the final test labels for fair comparison.","metadata":{}},{"cell_type":"code","source":"# Cast \"unsuitable image\" -> \"unsuitable\"\ndef clean_answer(word):\n    if word == \"unsuitable image\":\n        return \"unsuitable\"\n    else:\n        return word\ntrain_df[\"answer_list\"] = train_df[\"answer_list\"].apply(lambda ans:[clean_answer(w) for w in ans])\nval_df[\"answer_list\"] = val_df[\"answer_list\"].apply(lambda ans:[clean_answer(w) for w in ans])","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:49.250136Z","iopub.execute_input":"2022-09-15T09:20:49.252434Z","iopub.status.idle":"2022-09-15T09:20:49.371042Z","shell.execute_reply.started":"2022-09-15T09:20:49.252394Z","shell.execute_reply":"2022-09-15T09:20:49.369959Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Why is ANSWER_SPACE_SIZE defaulted to 3000?\nWe treat the VizWiz VQA task as a multiple-choice problem with a predefined answer space of the 3000 most frequent words, if the label doesn't exist in this set we assign it a **label \"\\<Unknown\\>\"**.\n\nWhen selecting 3000 as the ANSWER_SPACE_SIZE, 96.6% of the training data and 96.7% of the validation data has at least one answer in the answer space, and the max accuracy achievable on the validation set 88.13%. *(Note that the formula for VQA accuracy is different from standard accuracy)*. We see this as an acceptable trade off since increasing the answer space without additional data could make it more difficult for the model to learn and converge.","metadata":{}},{"cell_type":"code","source":"total_answer_counts = train_df[\"answer_list\"].append(val_df[\"answer_list\"]).explode().value_counts()\nprint(\"The total count of answer types are: {}\".format(total_answer_counts.shape))\nprint(\"Picking the top {} as answer space (and +1 for unknown).\".format(ANSWER_SPACE_SIZE))\n\nanswer_space = total_answer_counts.head(ANSWER_SPACE_SIZE).index.to_list()\nanswer_space.append(\"<Unknown>\")","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:49.372441Z","iopub.execute_input":"2022-09-15T09:20:49.372820Z","iopub.status.idle":"2022-09-15T09:20:49.450246Z","shell.execute_reply.started":"2022-09-15T09:20:49.372773Z","shell.execute_reply":"2022-09-15T09:20:49.448889Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"The total count of answer types are: (48729,)\nPicking the top 3000 as answer space (and +1 for unknown).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load external vqa data into dataset\n-----\nThe following script loads the COCO dataset to augment the training data. Is not used because it significantly prolongs the training time.","metadata":{}},{"cell_type":"code","source":"# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip -P data/\n# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip -P data/\n# !unzip -o -q ./data/v2_Questions_Train_mscoco.zip -d data/\n# !unzip -o -q ./data/v2_Annotations_Train_mscoco.zip -d data/","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:49.452140Z","iopub.execute_input":"2022-09-15T09:20:49.453040Z","iopub.status.idle":"2022-09-15T09:20:49.457966Z","shell.execute_reply.started":"2022-09-15T09:20:49.452999Z","shell.execute_reply":"2022-09-15T09:20:49.456682Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# VQA_IMAGES_TRAIN_PATH = \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/train2014\"\n# with open('./data/v2_OpenEnded_mscoco_train2014_questions.json') as fh:\n#     vqa_train_question_dict = json.load(fh)\n# with open('./data/v2_mscoco_train2014_annotations.json') as fh:\n#     vqa_train_annotation_dict = json.load(fh)\n    \n# vqa_image_base_path = Path(VQA_IMAGES_TRAIN_PATH)\n# vqa_train_question_df = pd.DataFrame(vqa_train_question_dict[\"questions\"])\n# vqa_train_annotation_df = pd.DataFrame(vqa_train_annotation_dict[\"annotations\"])\n# vqa_train_annotation_df = vqa_train_annotation_df.drop_duplicates(\"image_id\")\n# vqa_train_annotation_df[\"answer_list\"] = vqa_train_annotation_df[\"answers\"].apply(lambda row:[ans[\"answer\"] for ans in row])\n\n# vqa_train_df = vqa_train_annotation_df.join(vqa_train_question_df[[\"question\",\"question_id\"]].set_index(\"question_id\"),on=\"question_id\")\n# vqa_train_df[\"image_path\"] = vqa_train_df[\"image_id\"].apply(lambda i: str(vqa_image_base_path/\"COCO_train2014_{:012d}.jpg\".format(i)))\n\n# def most_common(lst):\n#     data = Counter(lst)\n#     return max(lst, key=data.get)\n# vqa_train_df[\"max_answer\"] = vqa_train_df[\"answers\"].apply(lambda row:most_common([ans[\"answer\"] for ans in row]))\n# vqa_train_df = vqa_train_df[[\"image_path\",\"question\",\"max_answer\",\"answer_list\"]]\n\n# train_df = train_df.append(vqa_train_df)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:49.462777Z","iopub.execute_input":"2022-09-15T09:20:49.463497Z","iopub.status.idle":"2022-09-15T09:20:49.469596Z","shell.execute_reply.started":"2022-09-15T09:20:49.463466Z","shell.execute_reply":"2022-09-15T09:20:49.468441Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"### Max_answer? Selected_answer?\nIf you read through the code line by line you might realize there is a max_answer field and wonder what the difference is. The max_answer field simply selects the most common answer within the ten given answers and is ok to use as the target label after filtering it to match the answer_space and is the first target label  we tried out. The problem is that it creates too much \"\\<Unknown\\>\" labels after filtering it with the answer_space which is not ideal.\n    \nSelected_answer below solves the problem by filtering the ten answers first, then picks the most common answer out of the valid answers. So even if only 1 out of the 10 answers is in the answer_space, we can at least assign it a target label that is weakly relevant instead of assigning it to an entirely irrelevant \"\\<Unknown\\>\" label. Using the selected_answer label does improve the performance.","metadata":{}},{"cell_type":"code","source":"from collections import Counter\ndef choose_ans_label(ans_list):\n    ans_dict = Counter(ans_list)\n    max_entry = \"<Unknown>\"\n    max_count = 0\n    for k,v in ans_dict.items():\n        if k in answer_space and v>max_count:\n            max_count = v\n            max_entry = k\n    return max_entry","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:49.470992Z","iopub.execute_input":"2022-09-15T09:20:49.471500Z","iopub.status.idle":"2022-09-15T09:20:49.483989Z","shell.execute_reply.started":"2022-09-15T09:20:49.471465Z","shell.execute_reply":"2022-09-15T09:20:49.482994Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df[\"selected_answer\"] = train_df[\"answer_list\"].apply(lambda row:choose_ans_label(row))\nval_df[\"selected_answer\"] = val_df[\"answer_list\"].apply(lambda row:choose_ans_label(row))","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:49.487142Z","iopub.execute_input":"2022-09-15T09:20:49.488503Z","iopub.status.idle":"2022-09-15T09:20:52.593943Z","shell.execute_reply.started":"2022-09-15T09:20:49.488467Z","shell.execute_reply":"2022-09-15T09:20:52.593001Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# encode labels\nfrom sklearn.preprocessing import OrdinalEncoder\nenc = OrdinalEncoder(categories = [answer_space[:-1]],handle_unknown=\"use_encoded_value\",unknown_value=ANSWER_SPACE_SIZE)\nenc.fit(train_df[\"selected_answer\"].values.reshape(-1,1))\ntrain_df[\"labels\"] = enc.transform(train_df[\"selected_answer\"].values.reshape(-1,1)).astype(int)\nval_df[\"labels\"] = enc.transform(val_df[\"selected_answer\"].values.reshape(-1,1)).astype(int)\n\n# remove unknown\n# keeping or removing unknown does not have significant effect if there isn't many <Unknowns>\ntrain_df = train_df[train_df[\"labels\"]!=ANSWER_SPACE_SIZE]\n\n# set index for val metrics\n# the indices here are accessed by the vqa_accuracy_score() to calculate validation metrics while training\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntrain_df[\"id\"] = train_df.index\nval_df[\"id\"] = val_df.index","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.595360Z","iopub.execute_input":"2022-09-15T09:20:52.595828Z","iopub.status.idle":"2022-09-15T09:20:52.654506Z","shell.execute_reply.started":"2022-09-15T09:20:52.595790Z","shell.execute_reply":"2022-09-15T09:20:52.653318Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.655896Z","iopub.execute_input":"2022-09-15T09:20:52.656928Z","iopub.status.idle":"2022-09-15T09:20:52.715802Z","shell.execute_reply.started":"2022-09-15T09:20:52.656891Z","shell.execute_reply":"2022-09-15T09:20:52.714851Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass MultimodalCollator:\n    tokenizer: AutoTokenizer\n    preprocessor: AutoFeatureExtractor\n\n    def tokenize_text(self, texts: List[str]):\n        encoded_text = self.tokenizer(\n            text=texts,\n            padding='longest',\n            max_length=68,\n            truncation=True,\n            return_tensors='pt',\n            return_token_type_ids=True,\n            return_attention_mask=True,\n        )\n        return {\n            \"input_ids\": encoded_text['input_ids'].squeeze(),\n            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n        }\n\n    def preprocess_images(self, images: List[str]):\n        processed_images = self.preprocessor(\n            images=[Image.open(image_path).convert('RGB') for image_path in images],\n            return_tensors=\"pt\",\n        )\n        return {\n            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n        }\n            \n    def __call__(self, raw_batch_dict):\n        return {\n            **self.tokenize_text(\n                raw_batch_dict['question']\n                if isinstance(raw_batch_dict, dict) else\n                [i['question'] for i in raw_batch_dict]\n            ),\n            **self.preprocess_images(\n                raw_batch_dict['image_path']\n                if isinstance(raw_batch_dict, dict) else\n                [i['image_path'] for i in raw_batch_dict]\n            ),\n            'labels': torch.tensor(\n                [raw_batch_dict['labels'],raw_batch_dict[\"id\"]]\n                if isinstance(raw_batch_dict, dict) else\n                [[i['labels'],i['id']] for i in raw_batch_dict],\n                dtype=torch.int64\n            ),\n        }","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.717371Z","iopub.execute_input":"2022-09-15T09:20:52.717747Z","iopub.status.idle":"2022-09-15T09:20:52.731514Z","shell.execute_reply.started":"2022-09-15T09:20:52.717716Z","shell.execute_reply":"2022-09-15T09:20:52.729626Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from typing import Optional, Sequence\n\nimport torch\nfrom torch import Tensor\nfrom torch import nn\nfrom torch.nn import functional as F\n\n# Based on https://github.com/AdeelH/pytorch-multi-class-focal-loss/blob/master/focal_loss.py\n# Added label smoothing\nclass FocalLoss(nn.Module):\n    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n    It is essentially an enhancement to cross entropy loss and is\n    useful for classification tasks when there is a large class imbalance.\n    x is expected to contain raw, unnormalized scores for each class.\n    y is expected to contain class labels.\n    Shape:\n        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n    \"\"\"\n\n    def __init__(self,\n                 alpha: Optional[Tensor] = None,\n                 gamma: float = 0.,\n                 reduction: str = 'mean',\n                 label_smoothing: float = 0.0,\n                 ignore_index: int = -100):\n        \"\"\"Constructor.\n        Args:\n            alpha (Tensor, optional): Weights for each class. Defaults to None.\n            gamma (float, optional): A constant, as described in the paper.\n                Defaults to 0.\n            reduction (str, optional): 'mean', 'sum' or 'none'.\n                Defaults to 'mean'.\n            ignore_index (int, optional): class label to ignore.\n                Defaults to -100.\n        \"\"\"\n        if reduction not in ('mean', 'sum', 'none'):\n            raise ValueError(\n                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.label_smoothing = label_smoothing\n\n        self.nll_loss = nn.NLLLoss(\n            weight=alpha, reduction='none', ignore_index=ignore_index)\n\n    def __repr__(self):\n        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n        arg_vals = [self.__dict__[k] for k in arg_keys]\n        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n        arg_str = ', '.join(arg_strs)\n        return f'{type(self).__name__}({arg_str})'\n\n    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n        if x.ndim > 2:\n            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n            c = x.shape[1]\n            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n            y = y.view(-1)\n\n        unignored_mask = y != self.ignore_index\n        y = y[unignored_mask]\n        if len(y) == 0:\n            return torch.tensor(0.)\n        x = x[unignored_mask]\n\n        # compute weighted cross entropy term: -alpha * log(pt)\n        # (alpha is already part of self.nll_loss)\n        log_p = F.log_softmax(x, dim=-1)\n        \n        if self.label_smoothing==0:\n            ce = self.nll_loss(log_p, y)\n        else:\n            ce = (1 - self.label_smoothing)*self.nll_loss(log_p, y) - self.label_smoothing * log_p.mean(dim=-1)\n\n        # get true class column from each row\n        all_rows = torch.arange(len(x))\n        log_pt = log_p[all_rows, y]\n\n        # compute focal term: (1 - pt)^gamma\n        pt = log_pt.exp()\n        focal_term = (1 - pt)**self.gamma\n\n        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n        loss = focal_term * ce\n\n        if self.reduction == 'mean':\n            loss = loss.mean()\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.733968Z","iopub.execute_input":"2022-09-15T09:20:52.734758Z","iopub.status.idle":"2022-09-15T09:20:52.750238Z","shell.execute_reply.started":"2022-09-15T09:20:52.734722Z","shell.execute_reply":"2022-09-15T09:20:52.749216Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"total_answer_counts = train_df[\"answer_list\"].append(val_df[\"answer_list\"]).explode().value_counts()\ntotal_answer_counts = total_answer_counts[total_answer_counts.index.isin(answer_space)][answer_space[:-1]]\n\n# freq_weight(alpha) is decided by trial and error and has no theory to support the decision\nfreq_weight = 1/np.log(total_answer_counts.head(ANSWER_SPACE_SIZE).values+10)\nfreq_weight = torch.tensor(np.append(freq_weight/np.mean(freq_weight),[0]),dtype=torch.float)\n\nfocal_loss = FocalLoss(\n        alpha=freq_weight,\n        gamma=2,\n        reduction='mean',\n        label_smoothing=0.0)\n\nprint(freq_weight)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.753175Z","iopub.execute_input":"2022-09-15T09:20:52.753432Z","iopub.status.idle":"2022-09-15T09:20:52.838781Z","shell.execute_reply.started":"2022-09-15T09:20:52.753408Z","shell.execute_reply":"2022-09-15T09:20:52.837704Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"tensor([0.3123, 0.3160, 0.3737,  ..., 1.1365, 1.1365, 0.0000])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Normal model\n-----","metadata":{}},{"cell_type":"code","source":"from torch.nn.utils.weight_norm import weight_norm\n\nclass MultimodalVQAModel(nn.Module):\n    def __init__(\n            self,\n            num_labels: int = len(answer_space),\n            intermediate_dim: int = 512,\n            pretrained_text_name: str = 'bert-base-uncased',\n            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n     \n        super(MultimodalVQAModel, self).__init__()\n        self.num_labels = num_labels\n        self.pretrained_text_name = pretrained_text_name\n        self.pretrained_image_name = pretrained_image_name\n        \n        self.text_encoder = AutoModel.from_pretrained(\n            self.pretrained_text_name,\n        )\n        self.image_encoder = AutoModel.from_pretrained(\n            self.pretrained_image_name,\n        )\n        \n#         for param in self.text_encoder.base_model.parameters():\n#             param.requires_grad = False\n#         for param in self.image_encoder.base_model.parameters():\n#             param.requires_grad = False\n        \n        # https://github.com/jiasenlu/vilbert_beta/blob/master/vilbert/basebert.py\n        hidden_size = self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size\n        intermediate_dim = hidden_size*2\n        self.fusion = nn.Sequential(\n            nn.Dropout(0.1),\n            weight_norm(nn.Linear(hidden_size, intermediate_dim)),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n        )\n        \n        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n        \n        for m in self.fusion:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_normal_(m.weight)\n                torch.nn.init.normal_(m.bias,std=0.03)\n        \n        try:\n            for m in self.classifier:\n                if isinstance(m, nn.Linear):\n                    torch.nn.init.kaiming_normal_(m.weight)\n                    torch.nn.init.normal_(m.bias,std=0.03)\n        except:\n            torch.nn.init.kaiming_normal_(self.classifier.weight)\n            torch.nn.init.normal_(self.classifier.bias,std=0.03)\n        \n        \n        self.criterion = focal_loss\n    \n    def forward(\n            self,\n            input_ids: torch.LongTensor,\n            pixel_values: torch.FloatTensor,\n            attention_mask: Optional[torch.LongTensor] = None,\n            token_type_ids: Optional[torch.LongTensor] = None,\n            labels: Optional[torch.LongTensor] = None):\n        \n        encoded_text = self.text_encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True,\n        )\n        encoded_image = self.image_encoder(\n            pixel_values=pixel_values,\n            return_dict=True,\n        )\n        \n        fused_output = self.fusion(\n            torch.cat(\n                [\n                    encoded_text['pooler_output'],\n                    encoded_image['pooler_output'],\n                ],\n                dim=1\n            )\n        )\n        logits = self.classifier(fused_output)\n        \n        out = {\n            \"logits\": logits\n        }\n        if labels is not None:\n            loss = self.criterion(logits, labels[:,0])\n            out[\"loss\"] = loss\n        \n        return out\n    \nprint(\"Loaded normal model\")","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.841573Z","iopub.execute_input":"2022-09-15T09:20:52.841879Z","iopub.status.idle":"2022-09-15T09:20:52.859128Z","shell.execute_reply.started":"2022-09-15T09:20:52.841852Z","shell.execute_reply":"2022-09-15T09:20:52.858098Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Loaded normal model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Choice embedding based\n-----\nIn order to run this model, the embeddings of the output choices must be generated from another script and loaded into this kaggle notebook with the path ANSWER_SPACE_EMBED_PATH due to memory constraints.\n\n```\n# Sample code to generate answer embeddings\nfrom torch.utils.data import DataLoader\nfrom transformers import RobertaTokenizer, RobertaModel\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\n\nword_dl = DataLoader(answer_space, batch_size=10, shuffle=False)\nmodel.to(device)\nanswer_embedding = []\nfor i,word_list in enumerate(word_dl):\n    encoded_input = tokenizer(word_list,padding=True, return_tensors='pt').to(device)\n    output = model(**encoded_input)\n    answer_embedding.append(output[\"pooler_output\"])\nanswer_tensor = torch.cat(answer_embedding,dim=0)\ntorch.save(answer_tensor, 'answer_embed.pt')\n```","metadata":{}},{"cell_type":"code","source":"%%script false --no-raise-error\n\n# The above magic skips this block when running\n\nfrom torch.nn.utils.weight_norm import weight_norm\n\nclass MultimodalVQAModel(nn.Module):\n    def __init__(\n            self,\n            num_labels: int = len(answer_space),\n            intermediate_dim: int = 512,\n            pretrained_text_name: str = 'bert-base-uncased',\n            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n     \n        super(MultimodalVQAModel, self).__init__()\n        self.num_labels = num_labels\n        self.pretrained_text_name = pretrained_text_name\n        self.pretrained_image_name = pretrained_image_name\n        \n        self.text_encoder = AutoModel.from_pretrained(\n            self.pretrained_text_name,\n        )\n        self.image_encoder = AutoModel.from_pretrained(\n            self.pretrained_image_name,\n        )\n        \n#         for param in self.text_encoder.base_model.parameters():\n#             param.requires_grad = False\n#         for param in self.image_encoder.base_model.parameters():\n#             param.requires_grad = False\n        \n        # https://github.com/jiasenlu/vilbert_beta/blob/master/vilbert/basebert.py\n        hidden_size = self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size\n        intermediate_dim = hidden_size*2\n        self.fusion = nn.Sequential(\n            nn.Dropout(0.1),\n#             weight_norm(nn.Linear(hidden_size, intermediate_dim)),\n            nn.Linear(hidden_size, intermediate_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(intermediate_dim, self.text_encoder.config.hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n        \n        for m in self.fusion:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_normal_(m.weight)\n                torch.nn.init.normal_(m.bias,std=0.03)\n        for m in self.classifier:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_normal_(m.weight)\n                torch.nn.init.normal_(m.bias,std=0.03)\n        \n        self.answer_embed_weights = nn.Parameter(torch.ones((3001,768)))\n        with torch.no_grad():\n            self.answer_embed_weights.copy_(torch.load(ANSWER_SPACE_EMBED_PATH))\n        print(self.answer_embed_weights)\n        self.output_bias = nn.Parameter(torch.zeros((self.num_labels,)))\n        nn.init.normal_(self.output_bias,std=0.03)\n        \n        self.criterion = focal_loss\n    \n    def forward(\n            self,\n            input_ids: torch.LongTensor,\n            pixel_values: torch.FloatTensor,\n            attention_mask: Optional[torch.LongTensor] = None,\n            token_type_ids: Optional[torch.LongTensor] = None,\n            labels: Optional[torch.LongTensor] = None):\n        \n        encoded_text = self.text_encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True,\n        )\n        encoded_image = self.image_encoder(\n            pixel_values=pixel_values,\n            return_dict=True,\n        )\n        \n        fused_output = self.fusion(\n            torch.cat(\n                [\n                    encoded_text['pooler_output'],\n                    encoded_image['pooler_output'],\n                ],\n                dim=1\n            )\n        )\n        model_embeddings = self.classifier(fused_output)\n\n        logits = (self.answer_embed_weights.unsqueeze(0) @ model_embeddings.unsqueeze(-1)).squeeze()+self.output_bias\n        \n        out = {\n            \"logits\": logits\n        }\n        if labels is not None:\n            loss = self.criterion(logits, labels[:,0])\n            out[\"loss\"] = loss\n        \n        return out\n\nprint(\"Loaded embedding model\")","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.861357Z","iopub.execute_input":"2022-09-15T09:20:52.861607Z","iopub.status.idle":"2022-09-15T09:20:52.902217Z","shell.execute_reply.started":"2022-09-15T09:20:52.861583Z","shell.execute_reply":"2022-09-15T09:20:52.901037Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def createMultimodalVQACollatorAndModel(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k'):\n    tokenizer = AutoTokenizer.from_pretrained(text)\n    preprocessor = AutoFeatureExtractor.from_pretrained(image)\n\n    multi_collator = MultimodalCollator(\n        tokenizer=tokenizer,\n        preprocessor=preprocessor,\n    )\n\n\n    multi_model = MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n#     print(multi_model)\n    return multi_collator, multi_model","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.905215Z","iopub.execute_input":"2022-09-15T09:20:52.905578Z","iopub.status.idle":"2022-09-15T09:20:52.911768Z","shell.execute_reply.started":"2022-09-15T09:20:52.905542Z","shell.execute_reply":"2022-09-15T09:20:52.910646Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def vqa_accuracy_score(val_id, preds):\n    acc_sum = 0\n    for v_id, pred in zip(val_id,preds):\n        pred_word = answer_space[pred]\n        answer_list = val_df.at[v_id,'answer_list']\n        acc_sum+=min(answer_list.count(pred_word)/3,1)\n    return acc_sum/len(preds)\n\ndef compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n    logits, labels = eval_tuple\n    preds = logits.argmax(axis=-1)\n    return {\n        \"acc\": accuracy_score(labels[:,0], preds),\n        \"vqa_acc\": vqa_accuracy_score(labels[:,1], preds),\n        \"f1\": f1_score(labels[:,0], preds, average='macro')\n    }","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.913521Z","iopub.execute_input":"2022-09-15T09:20:52.914188Z","iopub.status.idle":"2022-09-15T09:20:52.924812Z","shell.execute_reply.started":"2022-09-15T09:20:52.914152Z","shell.execute_reply":"2022-09-15T09:20:52.923719Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"answer_embed_into_model\",\n    seed=2022, \n#     evaluation_strategy=\"epoch\",\n    evaluation_strategy=\"steps\",\n    eval_steps=620,\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    save_strategy=\"steps\",\n    save_steps=620,\n#     save_strategy=\"epoch\",\n#     save_steps=100,\n    save_total_limit=1,             # Save only the last 1 checkpoints at any given time while training \n    metric_for_best_model='vqa_acc',\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    remove_unused_columns=False,\n    num_train_epochs=20,\n    fp16=True,\n    \n    dataloader_pin_memory=False,\n    \n    log_level=\"warning\",\n    \n    warmup_ratio=0.06,\n    learning_rate=1e-5,\n    weight_decay=0.1,\n    \n    dataloader_num_workers=2,\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.927865Z","iopub.execute_input":"2022-09-15T09:20:52.928336Z","iopub.status.idle":"2022-09-15T09:20:52.938393Z","shell.execute_reply.started":"2022-09-15T09:20:52.928304Z","shell.execute_reply":"2022-09-15T09:20:52.937444Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def createAndTrainModel(dataset, args, text_model='bert-base-uncased', image_model='google/vit-base-patch16-224-in21k', multimodal_model='bert_vit'):\n    collator, model = createMultimodalVQACollatorAndModel(text_model, image_model)\n    \n    multi_args = deepcopy(args)\n    multi_args.output_dir = os.path.join(\".\", \"checkpoint\", multimodal_model)\n    multi_trainer = Trainer(\n        model,\n        multi_args,\n        train_dataset=dataset['train'],\n        eval_dataset=dataset['val'],\n        data_collator=collator,\n        compute_metrics=compute_metrics\n    )\n    \n    train_multi_metrics = multi_trainer.train()\n    eval_multi_metrics = multi_trainer.evaluate()\n    # Remember to change this if you want to pass a different dataset to predict\n    test_results = multi_trainer.predict(dataset['val'])\n    \n    return collator, model, train_multi_metrics, eval_multi_metrics, test_results","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.940488Z","iopub.execute_input":"2022-09-15T09:20:52.941901Z","iopub.status.idle":"2022-09-15T09:20:52.950770Z","shell.execute_reply.started":"2022-09-15T09:20:52.941874Z","shell.execute_reply":"2022-09-15T09:20:52.949843Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"%env WANDB_PROJECT=vizwiz_results\n# Set the WANDB_API_KEY to avoid manually setting it every time \n# %env WANDB_API_KEY=\n%env WANDB_WATCH=all","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.953532Z","iopub.execute_input":"2022-09-15T09:20:52.953919Z","iopub.status.idle":"2022-09-15T09:20:52.965485Z","shell.execute_reply.started":"2022-09-15T09:20:52.953895Z","shell.execute_reply":"2022-09-15T09:20:52.964393Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"env: WANDB_PROJECT=vizwiz_results\nenv: WANDB_WATCH=all\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.init(name = \"Basic model\", notes=\"Basic model. dropout(0.1) lr(1e-5)\", save_code=True, reinit=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:20:52.970615Z","iopub.execute_input":"2022-09-15T09:20:52.971598Z","iopub.status.idle":"2022-09-15T09:21:17.886016Z","shell.execute_reply.started":"2022-09-15T09:20:52.971563Z","shell.execute_reply":"2022-09-15T09:21:17.884903Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220915_092114-1qfuvkz2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/lhanhsin/vizwiz_results/runs/1qfuvkz2\" target=\"_blank\">Basic model</a></strong> to <a href=\"https://wandb.ai/lhanhsin/vizwiz_results\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/lhanhsin/vizwiz_results/runs/1qfuvkz2?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f60d5fe0e10>"},"metadata":{}}]},{"cell_type":"code","source":"collator, model, train_multi_metrics, eval_multi_metrics, test_results = createAndTrainModel({\"train\": train_dataset,\"val\": val_dataset}, args,\n                                                                              text_model='roberta-base', image_model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")","metadata":{"execution":{"iopub.status.busy":"2022-09-15T09:21:17.888065Z","iopub.execute_input":"2022-09-15T09:21:17.888415Z","iopub.status.idle":"2022-09-15T10:35:31.433292Z","shell.execute_reply.started":"2022-09-15T09:21:17.888377Z","shell.execute_reply":"2022-09-15T10:35:31.428257Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c4fda0e52c4951afd13a0805eaa13c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec647e75c6664558859608ea3ad0750c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f78efdf4c59e4ee0aa9522c45083bcb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eab45ba0bb84edcadda8985aa6cb316"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/276 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e869b93572b848daaca40c2b4e2a4271"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"666ee4e8b50e4e7383d5cb9ed99b2463"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.59M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a97a6ba006446e3a17bca6eb2c39ee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/395M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"055670d011f44d1a8ceb92f7022f0977"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:2227.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 4.5343, 'learning_rate': 1.3440860215053765e-06, 'epoch': 0.16}\n{'loss': 4.4104, 'learning_rate': 2.688172043010753e-06, 'epoch': 0.32}\n{'loss': 3.9507, 'learning_rate': 4.018817204301075e-06, 'epoch': 0.48}\n{'loss': 3.6595, 'learning_rate': 5.362903225806452e-06, 'epoch': 0.65}\n{'loss': 3.567, 'learning_rate': 6.706989247311828e-06, 'epoch': 0.81}\n{'loss': 3.6632, 'learning_rate': 8.051075268817205e-06, 'epoch': 0.97}\n{'eval_loss': 2.961859941482544, 'eval_acc': 0.31095160916878906, 'eval_vqa_acc': 0.43466851894728614, 'eval_f1': 0.0013356211037317367, 'eval_runtime': 175.4358, 'eval_samples_per_second': 24.619, 'eval_steps_per_second': 0.77, 'epoch': 1.0}\n{'loss': 3.3609, 'learning_rate': 9.395161290322582e-06, 'epoch': 1.13}\n{'loss': 3.1942, 'learning_rate': 9.952814001372685e-06, 'epoch': 1.29}\n{'loss': 3.2396, 'learning_rate': 9.867021276595746e-06, 'epoch': 1.45}\n{'loss': 3.1683, 'learning_rate': 9.781228551818806e-06, 'epoch': 1.61}\n{'loss': 3.0269, 'learning_rate': 9.695435827041868e-06, 'epoch': 1.77}\n{'loss': 3.0004, 'learning_rate': 9.609643102264928e-06, 'epoch': 1.94}\n{'eval_loss': 2.7103540897369385, 'eval_acc': 0.362583931465617, 'eval_vqa_acc': 0.4797406807131265, 'eval_f1': 0.006107337295651353, 'eval_runtime': 176.1323, 'eval_samples_per_second': 24.521, 'eval_steps_per_second': 0.766, 'epoch': 2.0}\n{'loss': 2.9357, 'learning_rate': 9.52385037748799e-06, 'epoch': 2.1}\n{'loss': 2.8591, 'learning_rate': 9.43805765271105e-06, 'epoch': 2.26}\n{'loss': 2.7057, 'learning_rate': 9.352264927934111e-06, 'epoch': 2.42}\n{'loss': 2.7089, 'learning_rate': 9.266472203157173e-06, 'epoch': 2.58}\n{'loss': 2.6534, 'learning_rate': 9.180679478380235e-06, 'epoch': 2.74}\n{'loss': 2.644, 'learning_rate': 9.094886753603295e-06, 'epoch': 2.9}\n{'eval_loss': 2.5889334678649902, 'eval_acc': 0.3864320444547349, 'eval_vqa_acc': 0.5101489542332321, 'eval_f1': 0.012278754238150053, 'eval_runtime': 170.5239, 'eval_samples_per_second': 25.328, 'eval_steps_per_second': 0.792, 'epoch': 3.0}\n{'loss': 2.5778, 'learning_rate': 9.009094028826356e-06, 'epoch': 3.06}\n{'loss': 2.4047, 'learning_rate': 8.925017158544957e-06, 'epoch': 3.23}\n{'loss': 2.4647, 'learning_rate': 8.840082361015787e-06, 'epoch': 3.39}\n{'loss': 2.4008, 'learning_rate': 8.755147563486617e-06, 'epoch': 3.55}\n{'loss': 2.3266, 'learning_rate': 8.669354838709677e-06, 'epoch': 3.71}\n{'loss': 2.2384, 'learning_rate': 8.58356211393274e-06, 'epoch': 3.87}\n{'eval_loss': 2.5020337104797363, 'eval_acc': 0.4005556841861542, 'eval_vqa_acc': 0.5212626379563172, 'eval_f1': 0.026405964924019695, 'eval_runtime': 176.2287, 'eval_samples_per_second': 24.508, 'eval_steps_per_second': 0.766, 'epoch': 4.0}\n{'loss': 2.2565, 'learning_rate': 8.4977693891558e-06, 'epoch': 4.03}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/2194473103.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m collator, model, train_multi_metrics, eval_multi_metrics, test_results = createAndTrainModel({\"train\": train_dataset,\"val\": val_dataset}, args,\n\u001b[0;32m----> 2\u001b[0;31m                                                                               text_model='roberta-base', image_model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_17/1410428311.py\u001b[0m in \u001b[0;36mcreateAndTrainModel\u001b[0;34m(dataset, args, text_model, image_model, multimodal_model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_multi_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0meval_multi_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Remember to change this if you want to pass a different dataset to predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         )\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1721\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \"\"\"\n\u001b[0;32m-> 1535\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1559\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[0;32m-> 1561\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"val_df[\"prediction_label\"] = test_results.predictions.argmax(1)\nval_df[\"answer\"] = val_df[\"prediction_label\"].apply(lambda i: answer_space[i])\nval_df[\"image\"] = val_df[\"image_path\"].apply(lambda x:x.split(\"/\")[-1])\nval_df[[\"image\",\"answer\"]].to_json(\"./val_real_results.json\",orient = \"records\")\n\n# Saving predictions to wandb\nwandb.save('./val_real_results.json', policy=\"now\")\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T10:35:31.434983Z","iopub.status.idle":"2022-09-15T10:35:31.435467Z","shell.execute_reply.started":"2022-09-15T10:35:31.435213Z","shell.execute_reply":"2022-09-15T10:35:31.435236Z"},"trusted":true},"execution_count":null,"outputs":[]}]}